#!/usr/bin/env python3

import argparse
import csv
import glob
import json
import os
import shutil
import sys
import re


def jdump(x):
    """
    Convenience method for debugging.
    """
    print(json.dumps(x, indent=2))


def parse_excluded_runs_file(excluded_runs_file):
    """
    Parse the excluded runs file, return as a set.

    :param excluded_runs_file:
    :type excluded_runs_file: str
    :return: excluded_runs
    :rtype: set[str]
    """
    excluded_runs = set()
    with open(excluded_runs_file, 'r') as f:
        for line in f:
            excluded_runs.add(line.strip())

    return excluded_runs


def parse_excluded_libraries_file(excluded_libs_file):
    """
    Parse the excluded libraries file, return as a dict, indexed by sequencing run ID.

    :param excluded_libs_file:
    :type excluded_libs_file: str
    :return: excluded_libraries_by_run
    :rtype: dict[str, set[str]]
    """
    excluded_libraries_by_run = {}
    with open(excluded_libs_file, 'r') as f:
        for line in f:
            sequencing_run_id, library_id = line.strip().split(',')
            if sequencing_run_id not in excluded_libraries_by_run:
                excluded_libraries_by_run[sequencing_run_id] = set()
            excluded_libraries_by_run[sequencing_run_id].add(library_id)

    return excluded_libraries_by_run


def parse_config(config_path):
    """
    Parse json-format config file, return as a dict.

    :param config_path:
    :type config_path: str
    :return: config
    :rtype: dict[str, object]
    """
    config = {}
    if os.path.exists(config_path):
        with open(config_path, 'r') as f:
            try:
                config = json.load(f)
            except json.decoder.JSONDecodeError as e:
                print("ERROR: Error parsing config file: " + config_path, file=sys.stderr)
                print(e, file=sys.stderr)
                exit(-1)
        if 'excluded_runs_list' in config:
            config['excluded_runs'] = parse_excluded_runs_file(config['excluded_runs_list'])
        else:
            config['excluded_runs'] = set()

        if 'excluded_libraries_list' in config:
            config['excluded_libraries_by_sequencing_run_id'] = parse_excluded_libraries_file(config['excluded_libraries_list'])
        else:
            config['excluded_libraries_by_sequencing_run_id'] = {}
            
    else:
        print("ERROR: config file does not exist: " + config_path, file=sys.stderr)
        exit(-1)

    if 'sequencing_run_parent_dirs' not in config:
        print("ERROR: Config file missing required key 'sequencing_run_parent_dirs': " + config_path, file=sys.stderr)
        exit(-1)

    return config


def merge_config_with_args(config, args):
    """
    Over-write any config values supplied as command-line args.

    :param config: Parsed config from file.
    :type config: dict[str, object]
    :param args: command-line args.
    :type args: argparse.Namespace
    :return: merged config
    :rtype: dict[str, object]
    """
    if 'simplify_sample_id' not in config:
        config['simplify_sample_id'] = args.simplify_sample_id
    if 'outdir' not in config:
        config['outdir'] = args.outdir
    if 'copy' not in config:
        config['copy'] = args.copy
    if 'csv' not in config:
        config['csv'] = args.csv
    if 'project_id' not in config:
        config['project_id'] = args.project_id
    if 'skip_qc_status_check' not in config:
        config['skip_qc_status_check'] = args.skip_qc_status_check

    if args.excluded_run is not None:
        config['excluded_runs'].add(args.excluded_run)

    if args.excluded_runs_file is not None:
        excluded_runs_from_args = parse_excluded_runs_file(args.excluded_runs_file)
        config['excluded_runs'] = config['excluded_runs'].union(excluded_runs_from_args)

    if args.excluded_libraries_file is not None:
        excluded_libraries_from_args = parse_excluded_libraries_file(args.excluded_libraries_file)
        for run_id, libraries in excluded_libraries_from_args.items():
            if run_id not in config['excluded_libraries_by_sequencing_run_id']:
                config['excluded_libraries_by_sequencing_run_id'][run_id] = set()
            config['excluded_libraries_by_sequencing_run_id'][run_id] = config['excluded_libraries_by_sequencing_run_id'][run_id].union(libraries)

    return config


# https://stackoverflow.com/a/1176023
def camel_to_snake(name):
    """
    Convert camelCase strings to snake_case strings

    :param name: The name to convert
    :type name: str
    :return: snake_case name
    :rtype: str
    """
    name = re.sub('(.)([A-Z][a-z]+)', r'\1_\2', name)
    return re.sub('([a-z0-9])([A-Z])', r'\1_\2', name).lower()


def determine_sequencer_type(run_id):
    """
    Determine sequencer type based on format of Run ID
    Returns str ("miseq", "nextseq" or "gridion")

    :param run_id:
    :type run_id: str
    :return: sequencer type
    :rtype: str
    """
    miseq_regex = '\d{6}_M\d{5}_\d{4}_\d{9}-[A-Z0-9]{5}'
    nextseq_regex = '\d{6}_VH\d{5}_\d+_[A-Z0-9]{9}'
    gridion_regex = '\d{8}_\d{4}_X\d_[A-Z]{3}\d+_[a-z0-9]{8}'

    sequencer_type = None
    if re.match(miseq_regex, run_id):
        sequencer_type = "miseq"
    elif re.match(nextseq_regex, run_id):
        sequencer_type = "nextseq"
    elif re.match(gridion_regex, run_id):
        sequencer_type = "gridion"

    return sequencer_type


def determine_miseq_directory_structure(run_dir_path):
    """
    Determine whether a MiSeq run directory follows the "old" or "new" directory structure.
    In the "old" structure, fastq files are always found under "Data/Intensities/BaseCalls".
    In the "new" structure, fastq files are found under "Alignment_N/DATE_TIME/Fastq"

    :param run_dir_path:
    :type run_dir_path: str
    :return: "old" or "new"
    :rtype: str
    """

    run_dir_contents = os.listdir(run_dir_path)
    if "Alignment_1" in run_dir_contents:
        old_or_new = "new"
    else:
        old_or_new = "old"

    return old_or_new


def find_miseq_fastq_subdir(run_dir_path):
    """
    Find the sub-directory where fastq files are located.
    MiSeq runs may follow an 'old' or 'new' directory structure.

    :param run_dir_path:
    :type run_dir_path: str
    :return: fastq subdir
    :rtype: str
    """
    fastq_subdir = None
    directory_structure = determine_miseq_directory_structure(run_dir_path)
    if directory_structure == "old":
        fastq_subdir = "Data/Intensities/BaseCalls"
    elif directory_structure == "new":
        fastq_dirs = glob.glob(os.path.join(run_dir_path, 'Alignment_*', '*', 'Fastq'))
        try:
            fastq_dir = fastq_dirs[-1]
            fastq_subdir = os.path.relpath(fastq_dir, run_dir_path)
        except IndexError as e:
            print("WARNING: Failed to find fastq_directory in: " + run_dir_path, file=sys.stderr)
            fastq_subdir = None

    return fastq_subdir


def parse_samplesheet_miseq(samplesheet_path):
    """
    Parse a MiSeq SampleSheet to get sample and project IDs

    :param samplesheet_path: SampleSheet.csv file.
    :type samplesheet_path: str
    :return: List of sample records from SampleSheet.
    :rtype: list[dict[str, object]]
    """
    data = []
    with open(samplesheet_path, 'r') as f:
        for line in f:
            if not line.strip().startswith('[Data]'):
                continue
            else:
                break
          
        data_header = [x.lower() for x in next(f).strip().split(',')]
      
        for line in f:
            if not all([x == '' for x in line.strip().split(',')]):
                data_line = {}
                for idx, data_element in enumerate(line.strip().split(',')):
                    try:
                        data_line[data_header[idx]] = data_element
                    except IndexError as e:
                        pass
                data.append(data_line)

    return data


def parse_samplesheet_nextseq(samplesheet_path):
    """
    Parse a NextSeq SampleSheet to get sample and project IDs

    :param samplesheet_path: Path to SampleSheet.csv file.
    :type samplesheet_path: str
    :return: List of sample records from SampleSheet.
    :rtype: list[dict[str, object]]
    """
    cloud_data_lines = []
    cloud_data = []
    with open(samplesheet_path, 'r') as f:
        for line in f:
            if line.strip().startswith('[Cloud_Data]'):
                break
        for line in f:
            if line.strip().startswith('[Cloud_Settings]') or line.strip().rstrip(',') == "":
                break
            cloud_data_lines.append(line.strip().rstrip(','))

    if cloud_data_lines:
        cloud_data_keys = [camel_to_snake(x) for x in cloud_data_lines[0].split(',')]
        for line in cloud_data_lines[1:]:
            d = {}
            values = line.strip().split(',')
            if not all([x == '' for x in values]):
                for idx, key in enumerate(cloud_data_keys):
                    try:
                        d[key] = values[idx]
                    except IndexError as e:
                        d[key] = ""
                cloud_data.append(d)
  
    return cloud_data


def parse_samplesheet_gridion_custom(samplesheet_path):
    """
    Parse a custom GridION SampleSheet to get sample and project IDs.
    Custom GridION SampleSheets are simple .csv files that must have the following three fields:

    sample_id
    barcode
    project_id

    :param samplesheet_path:
    :type samplesheet_path: str
    :return: 
    :rtype:
    """
    samples = []
    with open(samplesheet_path, 'r') as f:
        reader = csv.DictReader(f)
        for row in reader:
            samples.append(row)

    return samples


def has_necessary_fields_for_symlinking_miseq(sample):
    """
    Check if a 'sample' dictionary has the necessary fields for symlinking
    Returns bool (True if ready for symlinking, false otherwise)

    :param sample:
    :type sample: dict[str, str]
    :return: Whether or not the sample has the appropriate fields necessary for symlinking
    :rtype bool:
    """
    selected = False
    conditions = []
    if 'sample_name' in sample and sample['sample_name'] != "":
        conditions.append(True)
    elif 'sample_id' in sample and sample['sample_id'] != "":
        sample['sample_name'] = sample['sample_id']
        conditions.append(True)
    else:
        conditions.append(False)

    if 'sample_project' in sample:
        conditions.append(True)
    else:
        conditions.append(False)

    if all(conditions):
        selected = True

    return selected


def has_necessary_fields_for_symlinking_nextseq(sample):
    """
    Check if a 'sample' dictionary has the necessary fields for symlinking
    Returns bool (True if ready for symlinking, false otherwise)

    :param sample:
    :type sample: dict[str, str]
    :return: Whether or not the sample has the appropriate fields necessary for symlinking
    :rtype bool:
    """
    selected = False
    conditions = []
    if 'sample_id' in sample and sample['sample_id'] != "":
        conditions.append(True)
    else:
        conditions.append(False)

    if 'project_name' in sample:
        conditions.append(True)
    else:
        conditions.append(False)

    if all(conditions):
        selected = True

    return selected


def has_necessary_fields_for_symlinking_gridion(sample):
    """
    Check if a 'sample' dictionary has the necessary fields for symlinking
    Returns bool (True if ready for symlinking, false otherwise)

    :param sample:
    :type sample: dict[str, str]
    :return: Whether or not the sample has the appropriate fields necessary for symlinking
    :rtype bool:
    """
    selected = False
    conditions = []
    if 'sample_id' in sample and sample['sample_id'] != "":
        conditions.append(True)
    else:
        conditions.append(False)

    if 'project_id' in sample:
        conditions.append(True)
    else:
        conditions.append(False)

    if all(conditions):
        selected = True

    return selected


def get_latest_analysis_subdir(run_dir):
    """
    For nextseq runs, there may be multiple `Analysis` sub-directories. We want to symlink to the files in the latest one.

    :param run_dir:
    :type run_dir: str
    :return: Path to latest analysis subdir, or None if the Analysis dir does not exist.
    :rtype: str|None
    """
    analysis_dir = os.path.join(os.path.join(run_dir, "Analysis"))
    if os.path.exists(analysis_dir):
        analysis_subdirs = os.listdir(analysis_dir)
        latest_analysis_subdir_num = analysis_subdirs[-1]
        latest_analysis_subdir = os.path.abspath(os.path.join(run_dir, "Analysis", latest_analysis_subdir_num))
    else:
        latest_analysis_subdir = None

    return latest_analysis_subdir
    
    return latest_analysis_subdir


def create_symlinks(src_dest_paths):
    """
    Symlink src to dest for each element in src_dest_paths

    :param src_dest_paths: list of dicts with keys "src" and "dest"
    :type src_dest_paths: list[dict[str, str]]
    :return: None
    :rtype: NoneType
    """
    for paths in src_dest_paths:
        outdir = os.path.dirname(paths['dest'])
        if not os.path.exists(outdir):
            os.makedirs(outdir)
        try:
            os.symlink(paths['src'], paths['dest'])
        except OSError as e:
            if e.errno == 17:
                pass
            else:
                print(str(e) + ". files: " + paths['src'] + ", " + paths['dest'], file=sys.stderr)


def create_copies(src_dest_paths):
    """
    Copy src to dest for each element in src_dest_paths

    :param src_dest_paths: list of dicts with keys "src" and "dest"
    :type src_dest_paths: list[dict[str, str]]
    :return: None
    :rtype: NoneType
    """
    for paths in src_dest_paths:
        outdir = os.path.dirname(paths['dest'])
        if not os.path.exists(outdir):
            os.makedirs(outdir)
        shutil.copy(paths['src'], paths['dest'])


def parse_ids_file(ids_file_path):
    """
    Parse the user-supplied sample IDs file.

    :param ids_file_path: Path to sample IDs file. Should contain one sample ID per line.
    :type ids_file_path: str
    :return: Set of sample IDs
    :rtype: set[str]
    """
    sample_ids = set()
    with open(ids_file_path, 'r') as f:
        for line in f:
            sample_ids.add(line.strip())

    return sample_ids


def simplify_fastq_filename(sample_id, original_filename):
    """
    Simplify fastq filename of destination symlink/copy by removing SXX_L001_R{1,2}_001, replacing simply with R1, R2 or RL.

    :param sample_id:
    :type sample_id: str
    :param original_filename:
    :type original_filename:
    :return:
    :rtype: str
    """
    read = ""
    if re.search('_R1_', original_filename):
        read = 'R1'
    elif re.search('_R2_', original_filename):
        read = 'R2'
    elif re.search('_RL', original_filename):
        read = 'RL'
    extension = '.fastq'
    if original_filename.split('.')[-1] == 'gz':
        extension += '.gz'

    dest_filename = sample_id + '_' + read + extension

    return dest_filename


def get_fastq_paths(config, run_dir, sample_ids, project_id):
    """
    Get paths to source and destination of fastq files.

    :param config:
    :type config: dict[str, object]
    :param run_dir:
    :type run_dir: str
    :param sample_ids:
    :type sample_ids: Iterable|NoneType
    :return: List of dicts with the keys: src, dest, sample_id
    :rtype: list[dict[str, str]]
    """

    run_id = os.path.basename(run_dir.rstrip('/'))
    sequencer_type = determine_sequencer_type(run_id)
    selected_samples = []
    fastq_subdir = None
    fastq_paths = []

    if sequencer_type == 'miseq':
        fastq_subdir = find_miseq_fastq_subdir(run_dir)
        samplesheet_path = os.path.join(run_dir, 'SampleSheet.csv')
        all_samples = parse_samplesheet_miseq(samplesheet_path)
        candidate_samples = list(filter(has_necessary_fields_for_symlinking_miseq, all_samples))
        # Occasionally sample IDs in the SampleSheet will include underscore characters, which are
        # automatically converted to dashes in the fastq filename. Need to convert them here prior to matching.
        for sample in candidate_samples:
            sample['sample_id'] = sample['sample_id'].replace('_', '-')
            sample['sample_name'] = sample['sample_name'].replace('_', '-')
        if sample_ids is not None:
            selected_samples = filter(lambda x: x['sample_id'] in sample_ids or x['sample_name'] in sample_ids, candidate_samples)
        elif project_id is not None:
            selected_samples = filter(lambda x: x['sample_project'] == project_id, candidate_samples)
        else:
            selected_samples = candidate_samples
        
    elif sequencer_type == 'nextseq':
        analysis_subdir = get_latest_analysis_subdir(run_dir)
        if analysis_subdir is not None:
            fastq_subdir = os.path.join(analysis_subdir, 'Data', 'fastq')
            samplesheet_paths = glob.glob(os.path.join(analysis_subdir, 'Data', 'SampleSheet*.csv'))
            if len(samplesheet_paths) > 0:
                samplesheet_path = samplesheet_paths[0] # If multiple samplesheets exist, how do we choose the correct one?
                all_samples = parse_samplesheet_nextseq(samplesheet_path)
            else:
                print("WARNING: Failed to find SampleSheet in: " + run_dir, file=sys.stderr)
                all_samples = []
            candidate_samples = list(filter(has_necessary_fields_for_symlinking_nextseq, all_samples))
            # Occasionally sample IDs in the SampleSheet will include underscore characters, which are
            # automatically converted to dashes in the fastq filename. Need to convert them here prior to matching.
            for sample in candidate_samples:
                sample['sample_id'] = sample['sample_id'].replace('_', '-')
            if sample_ids is not None:
                selected_samples = filter(lambda x: x['sample_id'] in sample_ids, candidate_samples)
            elif project_id is not None:
                selected_samples = filter(lambda x: x['project_name'] == project_id, candidate_samples)
            else:
                selected_samples = candidate_samples
        else:
            selected_samples = []

    elif sequencer_type == 'gridion':
        fastq_subdir = "fastq_pass_combined"
        samplesheet_paths = glob.glob(os.path.join(run_dir, 'SampleSheet*.csv'))
        if len(samplesheet_paths) > 0:
            samplesheet_path = samplesheet_paths[0]
            all_samples = parse_samplesheet_gridion_custom(samplesheet_path)
        else:
            print("WARNING: Failed to find SampleSheet in: " + run_dir, file=sys.stderr)
            all_samples = []
        candidate_samples = filter(has_necessary_fields_for_symlinking_gridion, all_samples)
        if sample_ids is not None:
            selected_samples = filter(lambda x: x['sample_id'] in sample_ids, candidate_samples)
        elif project_id is not None:
            selected_samples = filter(lambda x: x['project_name'] == project_id, candidate_samples)
        else:
            selected_samples = candidate_samples


    # Skip run if fastq_subdir cannot be determined
    if fastq_subdir is None:
        return []

    for sample in selected_samples:
        sample_id = None
        if sequencer_type == 'miseq':
            sample_id = sample['sample_name']
        elif sequencer_type == 'nextseq':
            sample_id = sample['sample_id']
        elif sequencer_type == 'gridion':
            sample_id = sample['sample_id']

        if sample_id is None:
            # Skip sample if sample_id cannot be determined
            continue

        sample_fastq_files = glob.glob(os.path.join(run_dir, fastq_subdir, sample_id + '_*.fastq'))
        sample_fastq_gz_files = glob.glob(os.path.join(run_dir, fastq_subdir, sample_id + '_*.fastq.gz'))
        for sample_fastq_file in sample_fastq_files + sample_fastq_gz_files:
            if os.path.exists(sample_fastq_file):
                src = os.path.abspath(sample_fastq_file)
                dest_filename = os.path.basename(sample_fastq_file)
                sample_id = dest_filename.split('_')[0]
                if config['simplify_sample_id']:
                    dest_filename = simplify_fastq_filename(sample_id, dest_filename)

                dest = os.path.join(config['outdir'], dest_filename)

                fastq_paths.append({"src": src, "dest": dest, "sample_id": sample_id})

    return fastq_paths


def determine_read_type(fastq_path):
    """
    Determine whether the fastq is an R1, R2 or RL file.

    :param fastq_paths:
    :type fastq_paths: str 
    :return: "R1" "R2" or "RL", or None if the read type cannot be determined.
    :rtype: str|None
    """
    read_type = None
    read_type_regex = "_(R[12L])[_\\.]"
    read_type_match = re.search(read_type_regex, fastq_path)
    if read_type_match:
        read_type = read_type_match.group(1)

    return read_type
    

def pair_fastq_paths(fastq_paths):
    """
    Determine whether the fastq is an R1, R2 or RL file.

    :param fastq_paths: List of dicts with fields: ['sample_id', 'src']
    :type fastq_paths: list[dict[str, str]]
    :return: List of dicts with fields: ['ID', 'R1', 'R2']
    :rtype: list[dict[str, str]]
    """
    paired_fastq_paths_by_sample_id = {}
    for fastq_path in fastq_paths:
        sample_id = fastq_path['sample_id']
        read_type = determine_read_type(fastq_path['src'])
        if sample_id not in paired_fastq_paths_by_sample_id:
            paired_fastq_paths_by_sample_id[sample_id] = {'ID': sample_id}
            paired_fastq_paths_by_sample_id[sample_id][read_type] = fastq_path['src']
        else:
            paired_fastq_paths_by_sample_id[sample_id][read_type] = fastq_path['src']

    paired_fastq_paths = sorted(paired_fastq_paths_by_sample_id.values(), key=lambda x: x['ID'])

    return paired_fastq_paths


def check_qc_status(run_dir):
    """
    Check the status of the QC check for a run.

    :param run_dir:
    :type run_dir: str
    :return: "PASS" or "FAIL" (or None if the qc_check_complete.json file does not exist)
    :rtype: str|None
    """
    qc_status = None
    qc_status_path = os.path.join(run_dir, 'qc_check_complete.json')
    if os.path.exists(qc_status_path):
        with open(qc_status_path, 'r') as f:
            qc_check = json.load(f)
            qc_status = qc_check['overall_pass_fail']
    else:
        print("WARNING: QC check file not found in: " + run_dir, file=sys.stderr)

    return qc_status


def main(args):

    config = {}
    config = parse_config(args.config)
    config = merge_config_with_args(config, args)

    sample_ids = None
    if args.ids_file is not None:
        sample_ids = parse_ids_file(args.ids_file)

    # In csv-mode, need to write the header once, before we start
    # looping over runs.
    if config['csv']:
        output_fieldnames = ['ID', 'R1', 'R2']
        writer = csv.DictWriter(sys.stdout, fieldnames=output_fieldnames)
        writer.writeheader()

    for run_parent_dir in config['sequencing_run_parent_dirs']:
        run_parent_dir_contents = list(map(lambda x: os.path.abspath(os.path.join(run_parent_dir, x)), os.listdir(run_parent_dir)))

        run_dirs = list(filter(lambda x: os.path.isdir(x), run_parent_dir_contents))

        # If a run ID is supplied by the `--run-id` flag, only symlink samples for that run.
        if args.run_id is not None:
            run_dirs = list(filter(lambda x: os.path.basename(x) == args.run_id, run_dirs))

        for run_dir in run_dirs:
            sequencing_run_id = os.path.basename(run_dir.rstrip('/'))
            if sequencing_run_id in config['excluded_runs']:
                print(f"INFO: Skipping run {sequencing_run_id} due to exclusion: {run_dir}", file=sys.stderr)
                continue
            if not config['skip_qc_status_check']:
                qc_status = check_qc_status(run_dir)
                if qc_status != None and qc_status != 'PASS':
                    print("WARNING: Skipping run due to failed QC: " + run_dir, file=sys.stderr)
                    continue

            included_sample_ids = []
            if sequencing_run_id in config['excluded_libraries_by_sequencing_run_id']:
                excluded_sample_ids = config['excluded_libraries_by_sequencing_run_id'][sequencing_run_id]
                for sample_id in sample_ids:
                    if sample_id not in excluded_sample_ids:
                        included_sample_ids.append(sample_id)
            else:
                included_sample_ids = sample_ids

            fastq_paths = get_fastq_paths(config, run_dir, included_sample_ids, args.project_id)

            if config['copy']:
                create_copies(fastq_paths)
            elif config['csv']:
                paired_fastq_paths = pair_fastq_paths(fastq_paths)
                if len(paired_fastq_paths) > 0:
                    output_fieldnames = ['ID', 'R1', 'R2']
                    writer = csv.DictWriter(sys.stdout, fieldnames=output_fieldnames)
                    for row in paired_fastq_paths:
                        writer.writerow(row)
            else:
                create_symlinks(fastq_paths)


if __name__ == '__main__':
    parser = argparse.ArgumentParser()
    parser.add_argument('-p', '--project-id')
    parser.add_argument('-r', '--run-id')
    parser.add_argument('-i', '--ids-file', help="File of sample IDs (one sample ID per line)")
    parser.add_argument('-s', '--simplify-sample-id', action='store_true', help="Simplify filenames of symlinks to include only sample-id_R{1,2}.fastq.gz")
    parser.add_argument('-c', '--config', default=os.path.expanduser('~/.config/symlink-seqs/config.json'), help="Config file (json format).")
    parser.add_argument('--copy', action='store_true', help="Create copies instead of symlinks.")
    parser.add_argument('--csv', action='store_true', help="Print csv-format summary of fastq file paths for each sample to stdout.")
    parser.add_argument('--skip-qc-status-check', action='store_true', help="Skip checking qc status for runs.")
    parser.add_argument('--exclude-run', help="A single run ID to exclude.")
    parser.add_argument('--excluded-runs-file', help="File containing list of run IDs to exclude. (single column, one run ID per line, no header)")
    parser.add_argument('--excluded-libraries-file', help="File containing list of run ID, library ID pairs to exclude. (csv format, no header)")
    parser.add_argument('-o', '--outdir', default='.', help="Output directory, where symlinks (or copies) will be created.")
    args = parser.parse_args()
    main(args)
